<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://tlardy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tlardy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-10-02T17:05:49+00:00</updated><id>https://tlardy.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates (Marta Cipriani)</title><link href="https://tlardy.github.io/talk/2024/10/02/cipriani/" rel="alternate" type="text/html" title="A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates (Marta Cipriani)"/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/10/02/cipriani</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/10/02/cipriani/"><![CDATA[<p><em> Medical advancements have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments in terms of both curing the disease and prolonging survival. We may use a Cox proportional hazards (PH) cure model to achieve this. However, a significant challenge in applying such a model is the potential presence of partially observed covariates in the data. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a more general case, where different covariate vectors are used to model the cure probability and the survival of patients who are not cured. We also propose an approximation of the exact conditional distribution using a regression approach, which helps draw imputed values at a lower computational cost. To assess its effectiveness, we compare the proposed approach with a complete case analysis and an analysis without any missing covariates. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Medical advancements have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments in terms of both curing the disease and prolonging survival. We may use a Cox proportional hazards (PH) cure model to achieve this. However, a significant challenge in applying such a model is the potential presence of partially observed covariates in the data. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a more general case, where different covariate vectors are used to model the cure probability and the survival of patients who are not cured. We also propose an approximation of the exact conditional distribution using a regression approach, which helps draw imputed values at a lower computational cost. To assess its effectiveness, we compare the proposed approach with a complete case analysis and an analysis without any missing covariates. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma.]]></summary></entry><entry><title type="html">Tight Bounds on Time-varying Causal Effects through a Causal Hazard Ratio (Jesus Garcia Garcia)</title><link href="https://tlardy.github.io/talk/2024/07/02/garcia/" rel="alternate" type="text/html" title="Tight Bounds on Time-varying Causal Effects through a Causal Hazard Ratio (Jesus Garcia Garcia)"/><published>2024-07-02T00:00:00+00:00</published><updated>2024-07-02T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/07/02/garcia</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/07/02/garcia/"><![CDATA[<p><em> A common question of causal nature in survival analysis is: Does a certain treatment have an effect on survival? Clinicians usually fit a Cox model, compute an estimated hazard ratio, and make their decisions based on this number alone. In the recent years, it has been shown that this is not enough to answer such a causal question, and the causal interpretation of this quantity has been a source of debate. An important issue is the time varying nature of the causal effect, which shows in a built-in survivor bias. This means that, even if treatment and control groups are similar at the beginning of the trial, the effect of treatment will make both groups more and more different as time goes on. There have been several solutions proposed, but we are going to choose one that defines a new causal version of the hazard ratio, that is not identifiable from data but gives a clear causal interpretation. From there, we will create a more general framework where tight bounds can be found for this quantity. We conclude with a practical example in vaccine efficacy, with a malaria and hiv vaccine trials data. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A common question of causal nature in survival analysis is: Does a certain treatment have an effect on survival? Clinicians usually fit a Cox model, compute an estimated hazard ratio, and make their decisions based on this number alone. In the recent years, it has been shown that this is not enough to answer such a causal question, and the causal interpretation of this quantity has been a source of debate. An important issue is the time varying nature of the causal effect, which shows in a built-in survivor bias. This means that, even if treatment and control groups are similar at the beginning of the trial, the effect of treatment will make both groups more and more different as time goes on. There have been several solutions proposed, but we are going to choose one that defines a new causal version of the hazard ratio, that is not identifiable from data but gives a clear causal interpretation. From there, we will create a more general framework where tight bounds can be found for this quantity. We conclude with a practical example in vaccine efficacy, with a malaria and hiv vaccine trials data.]]></summary></entry><entry><title type="html">Missing data in illness-death model: imputation methods and comparisons (Victor van der Horst)</title><link href="https://tlardy.github.io/talk/2024/06/18/horst/" rel="alternate" type="text/html" title="Missing data in illness-death model: imputation methods and comparisons (Victor van der Horst)"/><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/06/18/horst</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/06/18/horst/"><![CDATA[<p><em> Missing data arises in many data sets. Common imputation methods, such as the Multivariate Imputations by Chained Equations (MICE) and the multiple imputation by Substantive Model Compatible Fully Conditional Specification (SMC-FCS), have been worked out and analyzed for various settings, including competing risk analysis and traditional survival processes. In this talk, we will discuss the implementation for multi-state models, specifically the illness-death model, and compare different methods. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Missing data arises in many data sets. Common imputation methods, such as the Multivariate Imputations by Chained Equations (MICE) and the multiple imputation by Substantive Model Compatible Fully Conditional Specification (SMC-FCS), have been worked out and analyzed for various settings, including competing risk analysis and traditional survival processes. In this talk, we will discuss the implementation for multi-state models, specifically the illness-death model, and compare different methods.]]></summary></entry><entry><title type="html">Endogeneity in longitudinal modelling. A solution based on finite mixtures (Marco Alfo)</title><link href="https://tlardy.github.io/talk/2024/06/04/alfo/" rel="alternate" type="text/html" title="Endogeneity in longitudinal modelling. A solution based on finite mixtures (Marco Alfo)"/><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/06/04/alfo</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/06/04/alfo/"><![CDATA[<p><em> Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. We start from the so-called correlated effect approach, and argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. When the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further general solution may be considered. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. We start from the so-called correlated effect approach, and argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. When the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further general solution may be considered. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset.]]></summary></entry><entry><title type="html">Statistical modelling of time-stamped hypergraphs: a model-based clustering approach (Eugenia Driusso)</title><link href="https://tlardy.github.io/talk/2024/05/23/driusso/" rel="alternate" type="text/html" title="Statistical modelling of time-stamped hypergraphs: a model-based clustering approach (Eugenia Driusso)"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/05/23/driusso</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/05/23/driusso/"><![CDATA[<p><em> Throughout the years, several statistical models have been developed to describe interactions between individuals. These relational data are typically represented as a graph, where vertices correspond to the individuals, and two vertices are joined by an edge if there is a relationship among them. However, in some real-world situations, relationships may involve groups of individuals rather than just pairs. To represent such phenomena, an extension of graphs, called hypergraphs, can be utilized. Furthermore, relational data often evolve over time, but existing statistical methods for hypergraphs usually ignore this temporal information. To describe the evolution of hypergraphs over time, we propose a survival generalization of the Latent Class Analysis (LCA) approach for static hypergraphs. The proposed model is a mixture of Cox regression models for recurrent events. The model parameters are estimated via the Expectation-Maximization algorithm. This model represents the first attempt to model dynamic hypergraphs using a model-based clustering approach. Additionally, the proposed method has the potential to be the basis for future extensions and alternative modelling approaches, which will be briefly discussed. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Throughout the years, several statistical models have been developed to describe interactions between individuals. These relational data are typically represented as a graph, where vertices correspond to the individuals, and two vertices are joined by an edge if there is a relationship among them. However, in some real-world situations, relationships may involve groups of individuals rather than just pairs. To represent such phenomena, an extension of graphs, called hypergraphs, can be utilized. Furthermore, relational data often evolve over time, but existing statistical methods for hypergraphs usually ignore this temporal information. To describe the evolution of hypergraphs over time, we propose a survival generalization of the Latent Class Analysis (LCA) approach for static hypergraphs. The proposed model is a mixture of Cox regression models for recurrent events. The model parameters are estimated via the Expectation-Maximization algorithm. This model represents the first attempt to model dynamic hypergraphs using a model-based clustering approach. Additionally, the proposed method has the potential to be the basis for future extensions and alternative modelling approaches, which will be briefly discussed.]]></summary></entry><entry><title type="html">A Flexible Parametric Approach to Synthetic Patients Generation in Clinical Trials (Marta Cipriani)</title><link href="https://tlardy.github.io/talk/2024/05/21/cipriani/" rel="alternate" type="text/html" title="A Flexible Parametric Approach to Synthetic Patients Generation in Clinical Trials (Marta Cipriani)"/><published>2024-05-21T00:00:00+00:00</published><updated>2024-05-21T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/05/21/cipriani</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/05/21/cipriani/"><![CDATA[<p><em> Enhancing research reproducibility and data accessibility is essential in scientific research. However, ensuring data privacy while achieving these goals is a challenging task, especially in the medical field where sensitive data are often commonplace. One solution is to use synthetic data that mimic real-world datasets. This method has the potential to streamline therapy evaluation and facilitate quicker access to innovative treatments. We propose an approach that uses sequential conditional regression and flexible parametric survival models to accurately replicate covariate patterns and survival times. The approach is easily implementable through an R function. We also provide an application to an onco-hematological trial. The results show the potentialities of the proposed method in mirroring original distributions and survival outcomes. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Enhancing research reproducibility and data accessibility is essential in scientific research. However, ensuring data privacy while achieving these goals is a challenging task, especially in the medical field where sensitive data are often commonplace. One solution is to use synthetic data that mimic real-world datasets. This method has the potential to streamline therapy evaluation and facilitate quicker access to innovative treatments. We propose an approach that uses sequential conditional regression and flexible parametric survival models to accurately replicate covariate patterns and survival times. The approach is easily implementable through an R function. We also provide an application to an onco-hematological trial. The results show the potentialities of the proposed method in mirroring original distributions and survival outcomes.]]></summary></entry><entry><title type="html">Dirichlet Process Gaussian Mixture Models: an application in prostate cancer prediction (Dimitra Eleftheriou)</title><link href="https://tlardy.github.io/talk/2024/04/30/eleftheriou/" rel="alternate" type="text/html" title="Dirichlet Process Gaussian Mixture Models: an application in prostate cancer prediction (Dimitra Eleftheriou)"/><published>2024-04-30T00:00:00+00:00</published><updated>2024-04-30T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/04/30/eleftheriou</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/04/30/eleftheriou/"><![CDATA[<p><em> The Dirichlet Process (DP) is widely used in Bayesian non-parametrics, particularly in Dirichlet Process mixture models because of its flexibility and computational simplicity. The Dirichlet Process Gaussian mixture model (DP-GMM) is a limiting case of the finite Gaussian mixture model. This talk reviews DP-GMM and presents an extension of this model which accommodates covariates for multivariate density estimation and clustering problems. Emphasis is given to the main features of the DP mixture models, both with and without covariate information. Applications on real clinical data concerning prostate cancer are carried out using this methodology as an aim to construct an improved analytical tool for prostate cancer detection. Other relevant patient outcomes such as benign prostatic hyperplasia have also been considered. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[The Dirichlet Process (DP) is widely used in Bayesian non-parametrics, particularly in Dirichlet Process mixture models because of its flexibility and computational simplicity. The Dirichlet Process Gaussian mixture model (DP-GMM) is a limiting case of the finite Gaussian mixture model. This talk reviews DP-GMM and presents an extension of this model which accommodates covariates for multivariate density estimation and clustering problems. Emphasis is given to the main features of the DP mixture models, both with and without covariate information. Applications on real clinical data concerning prostate cancer are carried out using this methodology as an aim to construct an improved analytical tool for prostate cancer detection. Other relevant patient outcomes such as benign prostatic hyperplasia have also been considered.]]></summary></entry><entry><title type="html">Applying e-values to the logistic regression model (Henk van der Pol)</title><link href="https://tlardy.github.io/talk/2024/04/16/vdpol/" rel="alternate" type="text/html" title="Applying e-values to the logistic regression model (Henk van der Pol)"/><published>2024-04-16T00:00:00+00:00</published><updated>2024-04-16T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/04/16/vdpol</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/04/16/vdpol/"><![CDATA[<p><em> This thesis explores the theory of hypothesis testing for the logistic regression model using e-values. e-values have become an active subject of research to potentially replace the conventional and troubled p-value. Hypothesis testing using e-values provides a `safer’ inference by allowing to combine results from multiple tests, while preserving type-I error guarantee. We explore four methods to obtain e-values for the logistic regression model with two independent groups. We design e-values that are computationally efficient to compute and provide close to optimal evidence against the null hypothesis. In the first method, based on minimizing the Kullback-Leibler Divergence and depending on a limited subset of distributions in the alternative, we can compute e-values using a constant time algorithm by the Banach fixed-point Theorem. Next, by using the Rényi-Divergence, we can expand the subset of distributions in the alternative that provide e-values using the same constant time algorithm. However, these e-values do not provide close to optimal evidence against the null. Furthermore, with an algorithm due to J.Li, we can construct approximate e-values that provide an asymptotic close to optimal evidence against the null hypothesis. However, its considerable computation time makes it undesirable. Lastly, with a re-parametrization of the logistic regression model, we construct e-values in a simple setting that can be, again, computed in constant time. Nevertheless, despite them being designed to be close to optimal, we find that a simple existing e-test statistic provides more evidence if the null is not true. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[This thesis explores the theory of hypothesis testing for the logistic regression model using e-values. e-values have become an active subject of research to potentially replace the conventional and troubled p-value. Hypothesis testing using e-values provides a `safer’ inference by allowing to combine results from multiple tests, while preserving type-I error guarantee. We explore four methods to obtain e-values for the logistic regression model with two independent groups. We design e-values that are computationally efficient to compute and provide close to optimal evidence against the null hypothesis. In the first method, based on minimizing the Kullback-Leibler Divergence and depending on a limited subset of distributions in the alternative, we can compute e-values using a constant time algorithm by the Banach fixed-point Theorem. Next, by using the Rényi-Divergence, we can expand the subset of distributions in the alternative that provide e-values using the same constant time algorithm. However, these e-values do not provide close to optimal evidence against the null. Furthermore, with an algorithm due to J.Li, we can construct approximate e-values that provide an asymptotic close to optimal evidence against the null hypothesis. However, its considerable computation time makes it undesirable. Lastly, with a re-parametrization of the logistic regression model, we construct e-values in a simple setting that can be, again, computed in constant time. Nevertheless, despite them being designed to be close to optimal, we find that a simple existing e-test statistic provides more evidence if the null is not true.]]></summary></entry><entry><title type="html">Universal Reverse Information Projections and Optimal E-statistics (Tyron Lardy)</title><link href="https://tlardy.github.io/talk/2024/03/19/lardy/" rel="alternate" type="text/html" title="Universal Reverse Information Projections and Optimal E-statistics (Tyron Lardy)"/><published>2024-03-19T00:00:00+00:00</published><updated>2024-03-19T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/03/19/lardy</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/03/19/lardy/"><![CDATA[<p><em> Information projections have found important applications in probability theory, statistics, and related areas. In the field of hypothesis testing in particular, the reverse information projection (RIPr) has recently been shown to lead to so-called growth-rate optimal (GRO) e-statistics for testing simple alternatives against composite null hypotheses. However, the RIPr as well as the GRO criterion are undefined whenever the infimum information divergence between the null and alternative is infinite. We show that in such scenarios there often still exists an element in the alternative that is ‘closest’ to the null: the universal reverse information projection. The universal reverse information projection and its non-universal counterpart coincide whenever information divergence is finite. Furthermore, the universal RIPr is shown to lead to optimal e-statistics in a sense that is a novel, but natural, extension of the GRO criterion. We also give conditions under which the universal RIPr is a strict sub-probability distribution, as well as conditions under which an approximation of the universal RIPr leads to approximate e-statistics. For this case we provide tight relations between the corresponding approximation rates. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Information projections have found important applications in probability theory, statistics, and related areas. In the field of hypothesis testing in particular, the reverse information projection (RIPr) has recently been shown to lead to so-called growth-rate optimal (GRO) e-statistics for testing simple alternatives against composite null hypotheses. However, the RIPr as well as the GRO criterion are undefined whenever the infimum information divergence between the null and alternative is infinite. We show that in such scenarios there often still exists an element in the alternative that is ‘closest’ to the null: the universal reverse information projection. The universal reverse information projection and its non-universal counterpart coincide whenever information divergence is finite. Furthermore, the universal RIPr is shown to lead to optimal e-statistics in a sense that is a novel, but natural, extension of the GRO criterion. We also give conditions under which the universal RIPr is a strict sub-probability distribution, as well as conditions under which an approximation of the universal RIPr leads to approximate e-statistics. For this case we provide tight relations between the corresponding approximation rates.]]></summary></entry><entry><title type="html">PSA dynamics in metastatic prostate cancer: investigating the relationship between PSA trajectories and clinical benefit - a joint modeling and landmark approach (Lana Broer)</title><link href="https://tlardy.github.io/talk/2024/02/20/broer/" rel="alternate" type="text/html" title="PSA dynamics in metastatic prostate cancer: investigating the relationship between PSA trajectories and clinical benefit - a joint modeling and landmark approach (Lana Broer)"/><published>2024-02-20T00:00:00+00:00</published><updated>2024-02-20T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/02/20/broer</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/02/20/broer/"><![CDATA[<p><em> With prostate cancer being the most commonly diagnosed cancer in men worldwide, second in mortality and incidence only rising, this type of cancer is a substantial contributor to the cancer-related burden of disease. Prostate-specific antigen (PSA) is a pivotal biomarker in prostate cancer research, due to its high sensitivity and measurements being non-invasive. PSA has been researched mostly in diagnostic and recurrence settings, with less focus on its prognostic value. With this study we aimed to investigate the prognostic and predictive value of PSA in a metastatic prostate cancer population by exploring the relationship between PSA trajectories and no-longer clinical benefit (NLCB). NLCB describes the situation of progression where treatment is no longer effective. To quantify the prognostic value of PSA, different joint model specifications were explored. Two landmark approaches, consisting of last observation carried forward and penalized regression calibration models, were additionally considered for prediction. Data from the ProBio trial (Karolinska Institute, Sweden) – an international, phase-3 platform trial which aims to study novel treatments for metastatic prostate cancer – was used. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[With prostate cancer being the most commonly diagnosed cancer in men worldwide, second in mortality and incidence only rising, this type of cancer is a substantial contributor to the cancer-related burden of disease. Prostate-specific antigen (PSA) is a pivotal biomarker in prostate cancer research, due to its high sensitivity and measurements being non-invasive. PSA has been researched mostly in diagnostic and recurrence settings, with less focus on its prognostic value. With this study we aimed to investigate the prognostic and predictive value of PSA in a metastatic prostate cancer population by exploring the relationship between PSA trajectories and no-longer clinical benefit (NLCB). NLCB describes the situation of progression where treatment is no longer effective. To quantify the prognostic value of PSA, different joint model specifications were explored. Two landmark approaches, consisting of last observation carried forward and penalized regression calibration models, were additionally considered for prediction. Data from the ProBio trial (Karolinska Institute, Sweden) – an international, phase-3 platform trial which aims to study novel treatments for metastatic prostate cancer – was used.]]></summary></entry></feed>