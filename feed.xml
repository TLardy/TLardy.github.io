<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://tlardy.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tlardy.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-25T09:17:00+00:00</updated><id>https://tlardy.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Causal Inference with Outcomes Truncated by Death and Missing Not at Random (Yuan Liu)</title><link href="https://tlardy.github.io/talk/2025/02/11/liu/" rel="alternate" type="text/html" title="Causal Inference with Outcomes Truncated by Death and Missing Not at Random (Yuan Liu)"/><published>2025-02-11T00:00:00+00:00</published><updated>2025-02-11T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2025/02/11/liu</id><content type="html" xml:base="https://tlardy.github.io/talk/2025/02/11/liu/"><![CDATA[<p><em> In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study. </em></p> <p><em> You can find it also on <a href="https://arxiv.org/abs/2406.10554">https://arxiv.org/abs/2406.10554</a> </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In clinical trials, principal stratification analysis is commonly employed to address the issue of truncation by death, where a subject dies before the outcome can be measured. However, in practice, many survivor outcomes may remain uncollected or be missing not at random, posing a challenge to standard principal stratification analyses. In this paper, we explore the identification, estimation, and bounds of the average treatment effect within a subpopulation of individuals who would potentially survive under both treatment and control conditions. We show that the causal parameter of interest can be identified by introducing a proxy variable that affects the outcome only through the principal strata, while requiring that the treatment variable does not directly affect the missingness mechanism. Subsequently, we propose an approach for estimating causal parameters and derive nonparametric bounds in cases where identification assumptions are violated. We illustrate the performance of the proposed method through simulation studies and a real dataset obtained from a Human Immunodeficiency Virus (HIV) study.]]></summary></entry><entry><title type="html">[To be rescheduled] Limiting Behaviours for Maximum of Weighted Sums of WOD Random Variables and its Applications (Tianjiao Yan)</title><link href="https://tlardy.github.io/talk/2025/01/28/yan/" rel="alternate" type="text/html" title="[To be rescheduled] Limiting Behaviours for Maximum of Weighted Sums of WOD Random Variables and its Applications (Tianjiao Yan)"/><published>2025-01-28T00:00:00+00:00</published><updated>2025-01-28T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2025/01/28/yan</id><content type="html" xml:base="https://tlardy.github.io/talk/2025/01/28/yan/"><![CDATA[<p><em> Probability limit theory for independent random variables has produced many significant results. However, in practical applications, the assumption of independence among variables is often difficult to satisfy. Therefore, the study of the limiting behaviour of dependent random variables is of great significance. This research focuses on the limiting behaviour of maximum of randomly weighted sums for a sequence of random variables that satisfy WOD structure. The findings extended existing theoretical results. Based on the obtained results, the Marcinkiewicz-Zygmund type strong law of large numbers is established, and the strong consistency of estimators in certain statistical models is proved. To support the theoretical results, the simulation work to verify the validity of the theoretical results is also conducted. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Probability limit theory for independent random variables has produced many significant results. However, in practical applications, the assumption of independence among variables is often difficult to satisfy. Therefore, the study of the limiting behaviour of dependent random variables is of great significance. This research focuses on the limiting behaviour of maximum of randomly weighted sums for a sequence of random variables that satisfy WOD structure. The findings extended existing theoretical results. Based on the obtained results, the Marcinkiewicz-Zygmund type strong law of large numbers is established, and the strong consistency of estimators in certain statistical models is proved. To support the theoretical results, the simulation work to verify the validity of the theoretical results is also conducted.]]></summary></entry><entry><title type="html">A general approach to fitting multistate cure models based on an extended-long-format data structure (Astrid Stulemeijer)</title><link href="https://tlardy.github.io/talk/2024/11/26/stulemeijer/" rel="alternate" type="text/html" title="A general approach to fitting multistate cure models based on an extended-long-format data structure (Astrid Stulemeijer)"/><published>2024-11-26T00:00:00+00:00</published><updated>2024-11-26T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/11/26/stulemeijer</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/11/26/stulemeijer/"><![CDATA[<p><em> In recent years, the analysis of longitudinal (social) network data using Stochastic Actor-Oriented Models (SAOMs) has become increasingly popular across a variety of domains. SAOMs offer insight into the dynamics that drive network evolution by modelling multiple, discrete observations of sociocentric network data as coming from a continuous-time Markov chain. However, as with all survey-based data, social network data are susceptible to measurement errors, which may bias the parameter estimates. This talk presents a simulation study that assesses the effects of measurement errors on the results of Stochastic Actor-Oriented Models. Both randomly introduced measurement errors and systematically introduced measurement error informed by social and cognitive sciences are considered. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[In recent years, the analysis of longitudinal (social) network data using Stochastic Actor-Oriented Models (SAOMs) has become increasingly popular across a variety of domains. SAOMs offer insight into the dynamics that drive network evolution by modelling multiple, discrete observations of sociocentric network data as coming from a continuous-time Markov chain. However, as with all survey-based data, social network data are susceptible to measurement errors, which may bias the parameter estimates. This talk presents a simulation study that assesses the effects of measurement errors on the results of Stochastic Actor-Oriented Models. Both randomly introduced measurement errors and systematically introduced measurement error informed by social and cognitive sciences are considered.]]></summary></entry><entry><title type="html">A general approach to fitting multistate cure models based on an extended-long-format data structure (Yilin Jiang)</title><link href="https://tlardy.github.io/talk/2024/10/29/jiang/" rel="alternate" type="text/html" title="A general approach to fitting multistate cure models based on an extended-long-format data structure (Yilin Jiang)"/><published>2024-10-29T00:00:00+00:00</published><updated>2024-10-29T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/10/29/jiang</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/10/29/jiang/"><![CDATA[<p><em> A multistate cure model is a statistical framework used to analyze and represent the transitions that individuals undergo between different states over time, taking into account the possibility of being cured by initial treatment. This model is particularly useful in pediatric oncology where a proportion of the patient population achieves cure through treatment and therefore they will never experience some events. Our study develops a generalized algorithm based on the extended long data format, an extension of long data format where a transition can be split up to two rows each with a weight assigned reflecting the posterior probability of its cure status. The multistate cure model is fit on top of the current framework of multistate model and mixture cure model. The proposed algorithm makes use of the Expectation-Maximization (EM) algorithm and weighted likelihood representation such that it is easy to implement with standard package. As an example, the proposed algorithm is applied on data from the European Society for Blood and Marrow Transplantation (EBMT). </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A multistate cure model is a statistical framework used to analyze and represent the transitions that individuals undergo between different states over time, taking into account the possibility of being cured by initial treatment. This model is particularly useful in pediatric oncology where a proportion of the patient population achieves cure through treatment and therefore they will never experience some events. Our study develops a generalized algorithm based on the extended long data format, an extension of long data format where a transition can be split up to two rows each with a weight assigned reflecting the posterior probability of its cure status. The multistate cure model is fit on top of the current framework of multistate model and mixture cure model. The proposed algorithm makes use of the Expectation-Maximization (EM) algorithm and weighted likelihood representation such that it is easy to implement with standard package. As an example, the proposed algorithm is applied on data from the European Society for Blood and Marrow Transplantation (EBMT).]]></summary></entry><entry><title type="html">Non-parametric estimation of transition intensities in interval censored Markov multi-state models without loops (Daniel Gomon)</title><link href="https://tlardy.github.io/talk/2024/10/15/gomon/" rel="alternate" type="text/html" title="Non-parametric estimation of transition intensities in interval censored Markov multi-state models without loops (Daniel Gomon)"/><published>2024-10-15T00:00:00+00:00</published><updated>2024-10-15T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/10/15/gomon</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/10/15/gomon/"><![CDATA[<p><em> Panel data arises when transitions between different states are interval-censored in multi-state data. The analysis of such data using non-parametric multi-state models was not possible until recently, but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a non-parametric estimator of the transition intensities for panel data using an Expectation Maximisation algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm to the non-parametric maximum likelihood estimator is given. A simulation study comparing the proposed estimator to a consistent estimator is performed, and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analysed. Code to perform the analyses is publicly available. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Panel data arises when transitions between different states are interval-censored in multi-state data. The analysis of such data using non-parametric multi-state models was not possible until recently, but is very desirable as it allows for more flexibility than its parametric counterparts. The single available result to date has some unique drawbacks. We propose a non-parametric estimator of the transition intensities for panel data using an Expectation Maximisation algorithm. The method allows for a mix of interval-censored and right-censored (exactly observed) transitions. A condition to check for the convergence of the algorithm to the non-parametric maximum likelihood estimator is given. A simulation study comparing the proposed estimator to a consistent estimator is performed, and shown to yield near identical estimates at smaller computational cost. A data set on the emergence of teeth in children is analysed. Code to perform the analyses is publicly available.]]></summary></entry><entry><title type="html">A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates (Marta Cipriani)</title><link href="https://tlardy.github.io/talk/2024/10/02/cipriani/" rel="alternate" type="text/html" title="A multiple imputation approach to distinguish curative from life-prolonging effects in the presence of missing covariates (Marta Cipriani)"/><published>2024-10-02T00:00:00+00:00</published><updated>2024-10-02T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/10/02/cipriani</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/10/02/cipriani/"><![CDATA[<p><em> Medical advancements have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments in terms of both curing the disease and prolonging survival. We may use a Cox proportional hazards (PH) cure model to achieve this. However, a significant challenge in applying such a model is the potential presence of partially observed covariates in the data. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a more general case, where different covariate vectors are used to model the cure probability and the survival of patients who are not cured. We also propose an approximation of the exact conditional distribution using a regression approach, which helps draw imputed values at a lower computational cost. To assess its effectiveness, we compare the proposed approach with a complete case analysis and an analysis without any missing covariates. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Medical advancements have increased cancer survival rates and the possibility of finding a cure. Hence, it is crucial to evaluate the impact of treatments in terms of both curing the disease and prolonging survival. We may use a Cox proportional hazards (PH) cure model to achieve this. However, a significant challenge in applying such a model is the potential presence of partially observed covariates in the data. We aim to refine the methods for imputing partially observed covariates based on multiple imputation and fully conditional specification (FCS) approaches. To be more specific, we consider a more general case, where different covariate vectors are used to model the cure probability and the survival of patients who are not cured. We also propose an approximation of the exact conditional distribution using a regression approach, which helps draw imputed values at a lower computational cost. To assess its effectiveness, we compare the proposed approach with a complete case analysis and an analysis without any missing covariates. We discuss the application of these techniques to a real-world dataset from the BO06 clinical trial on osteosarcoma.]]></summary></entry><entry><title type="html">Tight Bounds on Time-varying Causal Effects through a Causal Hazard Ratio (Jesus Garcia Garcia)</title><link href="https://tlardy.github.io/talk/2024/07/02/garcia/" rel="alternate" type="text/html" title="Tight Bounds on Time-varying Causal Effects through a Causal Hazard Ratio (Jesus Garcia Garcia)"/><published>2024-07-02T00:00:00+00:00</published><updated>2024-07-02T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/07/02/garcia</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/07/02/garcia/"><![CDATA[<p><em> A common question of causal nature in survival analysis is: Does a certain treatment have an effect on survival? Clinicians usually fit a Cox model, compute an estimated hazard ratio, and make their decisions based on this number alone. In the recent years, it has been shown that this is not enough to answer such a causal question, and the causal interpretation of this quantity has been a source of debate. An important issue is the time varying nature of the causal effect, which shows in a built-in survivor bias. This means that, even if treatment and control groups are similar at the beginning of the trial, the effect of treatment will make both groups more and more different as time goes on. There have been several solutions proposed, but we are going to choose one that defines a new causal version of the hazard ratio, that is not identifiable from data but gives a clear causal interpretation. From there, we will create a more general framework where tight bounds can be found for this quantity. We conclude with a practical example in vaccine efficacy, with a malaria and hiv vaccine trials data. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[A common question of causal nature in survival analysis is: Does a certain treatment have an effect on survival? Clinicians usually fit a Cox model, compute an estimated hazard ratio, and make their decisions based on this number alone. In the recent years, it has been shown that this is not enough to answer such a causal question, and the causal interpretation of this quantity has been a source of debate. An important issue is the time varying nature of the causal effect, which shows in a built-in survivor bias. This means that, even if treatment and control groups are similar at the beginning of the trial, the effect of treatment will make both groups more and more different as time goes on. There have been several solutions proposed, but we are going to choose one that defines a new causal version of the hazard ratio, that is not identifiable from data but gives a clear causal interpretation. From there, we will create a more general framework where tight bounds can be found for this quantity. We conclude with a practical example in vaccine efficacy, with a malaria and hiv vaccine trials data.]]></summary></entry><entry><title type="html">Missing data in illness-death model: imputation methods and comparisons (Victor van der Horst)</title><link href="https://tlardy.github.io/talk/2024/06/18/horst/" rel="alternate" type="text/html" title="Missing data in illness-death model: imputation methods and comparisons (Victor van der Horst)"/><published>2024-06-18T00:00:00+00:00</published><updated>2024-06-18T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/06/18/horst</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/06/18/horst/"><![CDATA[<p><em> Missing data arises in many data sets. Common imputation methods, such as the Multivariate Imputations by Chained Equations (MICE) and the multiple imputation by Substantive Model Compatible Fully Conditional Specification (SMC-FCS), have been worked out and analyzed for various settings, including competing risk analysis and traditional survival processes. In this talk, we will discuss the implementation for multi-state models, specifically the illness-death model, and compare different methods. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Missing data arises in many data sets. Common imputation methods, such as the Multivariate Imputations by Chained Equations (MICE) and the multiple imputation by Substantive Model Compatible Fully Conditional Specification (SMC-FCS), have been worked out and analyzed for various settings, including competing risk analysis and traditional survival processes. In this talk, we will discuss the implementation for multi-state models, specifically the illness-death model, and compare different methods.]]></summary></entry><entry><title type="html">Endogeneity in longitudinal modelling. A solution based on finite mixtures (Marco Alfo)</title><link href="https://tlardy.github.io/talk/2024/06/04/alfo/" rel="alternate" type="text/html" title="Endogeneity in longitudinal modelling. A solution based on finite mixtures (Marco Alfo)"/><published>2024-06-04T00:00:00+00:00</published><updated>2024-06-04T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/06/04/alfo</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/06/04/alfo/"><![CDATA[<p><em> Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. We start from the so-called correlated effect approach, and argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. When the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further general solution may be considered. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Individual-specific, time-constant, random effects are often used to model dependence and/or to account for omitted covariates in regression models for longitudinal responses. Longitudinal studies have known a huge and widespread use in the last few years as they allow to distinguish between so-called age and cohort effects; these relate to differences that can be observed at the beginning of the study and stay persistent through time, and changes in the response that are due to the temporal dynamics in the observed covariates. While there is a clear and general agreement on this purpose, the random effect approach has been frequently criticized for not being robust to the presence of correlation between the observed (i.e. covariates) and the unobserved (i.e. random effects) heterogeneity. We start from the so-called correlated effect approach, and argue that the random effect approach may be parametrized to account for potential correlation between observables and unobservables. When the random effect distribution is estimated non-parametrically using a discrete distribution on finite number of locations, a further general solution may be considered. This is illustrated via a large scale simulation study and the analysis of a benchmark dataset.]]></summary></entry><entry><title type="html">Statistical modelling of time-stamped hypergraphs: a model-based clustering approach (Eugenia Driusso)</title><link href="https://tlardy.github.io/talk/2024/05/23/driusso/" rel="alternate" type="text/html" title="Statistical modelling of time-stamped hypergraphs: a model-based clustering approach (Eugenia Driusso)"/><published>2024-05-23T00:00:00+00:00</published><updated>2024-05-23T00:00:00+00:00</updated><id>https://tlardy.github.io/talk/2024/05/23/driusso</id><content type="html" xml:base="https://tlardy.github.io/talk/2024/05/23/driusso/"><![CDATA[<p><em> Throughout the years, several statistical models have been developed to describe interactions between individuals. These relational data are typically represented as a graph, where vertices correspond to the individuals, and two vertices are joined by an edge if there is a relationship among them. However, in some real-world situations, relationships may involve groups of individuals rather than just pairs. To represent such phenomena, an extension of graphs, called hypergraphs, can be utilized. Furthermore, relational data often evolve over time, but existing statistical methods for hypergraphs usually ignore this temporal information. To describe the evolution of hypergraphs over time, we propose a survival generalization of the Latent Class Analysis (LCA) approach for static hypergraphs. The proposed model is a mixture of Cox regression models for recurrent events. The model parameters are estimated via the Expectation-Maximization algorithm. This model represents the first attempt to model dynamic hypergraphs using a model-based clustering approach. Additionally, the proposed method has the potential to be the basis for future extensions and alternative modelling approaches, which will be briefly discussed. </em></p>]]></content><author><name></name></author><summary type="html"><![CDATA[Throughout the years, several statistical models have been developed to describe interactions between individuals. These relational data are typically represented as a graph, where vertices correspond to the individuals, and two vertices are joined by an edge if there is a relationship among them. However, in some real-world situations, relationships may involve groups of individuals rather than just pairs. To represent such phenomena, an extension of graphs, called hypergraphs, can be utilized. Furthermore, relational data often evolve over time, but existing statistical methods for hypergraphs usually ignore this temporal information. To describe the evolution of hypergraphs over time, we propose a survival generalization of the Latent Class Analysis (LCA) approach for static hypergraphs. The proposed model is a mixture of Cox regression models for recurrent events. The model parameters are estimated via the Expectation-Maximization algorithm. This model represents the first attempt to model dynamic hypergraphs using a model-based clustering approach. Additionally, the proposed method has the potential to be the basis for future extensions and alternative modelling approaches, which will be briefly discussed.]]></summary></entry></feed>