---
layout: post
title: "Applying e-values to the logistic regression model (Henk van der Pol)"
location: "BM.2.26 (2nd floor B-wing of the Gorlaeus)"
time: "13:00 - 14:00"
---

<em>
This thesis explores the theory of hypothesis testing for the logistic regression model using e-values. e-values have become an active subject of research to potentially replace the conventional and troubled p-value. Hypothesis testing using e-values provides a `safer' inference by allowing to combine results from multiple tests, while preserving type-I error guarantee. We explore four methods to obtain e-values for the logistic regression model with two independent groups. We design e-values that are computationally efficient to compute and provide close to optimal evidence against the null hypothesis. In the first method, based on minimizing the Kullback-Leibler Divergence and depending on a limited subset of distributions in the alternative, we can compute e-values using a constant time algorithm by the Banach fixed-point Theorem. Next, by using the RÃ©nyi-Divergence, we can expand the subset of distributions in the alternative that provide e-values using the same constant time algorithm. However, these e-values do not provide close to optimal evidence against the null. Furthermore, with an algorithm due to J.Li, we can construct approximate e-values that provide an asymptotic close to optimal evidence against the null hypothesis. However, its considerable computation time makes it undesirable. Lastly, with a re-parametrization of the logistic regression model, we construct e-values in a simple setting that can be, again, computed in constant time. Nevertheless, despite them being designed to be close to optimal, we find that a simple existing e-test statistic provides more evidence if the null is not true.
</em>
